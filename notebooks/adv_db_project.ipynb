{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e622054-8136-411f-b0ef-cd2cb8d919db",
   "metadata": {},
   "source": [
    "# Advanced DB Project 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cced8-83ab-404b-98fb-a2485156a4cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd83e0d3-f2f0-4689-9bc0-b122e7e4211d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>34</td><td>application_1738075734771_0035</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0035/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0035_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>15</td><td>application_1738075734771_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0016_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>18</td><td>application_1738075734771_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0019_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>21</td><td>application_1738075734771_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0022_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>22</td><td>application_1738075734771_0023</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0023/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0023_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>26</td><td>application_1738075734771_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0027_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>30</td><td>application_1738075734771_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0031_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>32</td><td>application_1738075734771_0033</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0033/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0033_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>34</td><td>application_1738075734771_0035</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0035/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0035_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d225d0a-fa4b-4eda-b1aa-f49e3f616698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 4\n",
      "[['191522756', '12/31/2019 12:00:00 AM', '10/19/2019 12:00:00 AM', '1500', '15', 'N Hollywood', '1526', '1', '235', 'CHILD ABUSE (PHYSICAL) - AGGRAVATED ASSAULT', '0913 0334 0449 0416 0419 0552 0400 0408', '16', 'F', 'W', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '205', 'KITCHEN KNIFE', 'IC', 'Invest Cont', '235', '', '', '', '6000    HAZELHURST                   PL', '', '34.1794', '-118.3856']]\n",
      "[['240404002', '12/31/2023 12:00:00 AM', '12/31/2023 12:00:00 AM', '1700', '04', 'Hollenbeck', '0477', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1813 0400 0411', '21', 'M', 'H', '501', 'SINGLE FAMILY DWELLING', '208', 'RAZOR', 'IC', 'Invest Cont', '236', '', '', '', 'WHITTIER', 'SOTO', '34.0344', '-118.2157']]\n",
      "[['240204018', '12/31/2023 12:00:00 AM', '12/31/2023 12:00:00 AM', '1600', '02', 'Rampart', '0233', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0416 1813 0913 2002', '26', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '', '', '', '200 S  LA FAYETTE PARK              PL', '', '34.0693', '-118.2792']]\n",
      "[['231615499', '12/31/2023 12:00:00 AM', '12/30/2023 12:00:00 AM', '0600', '16', 'Foothill', '1676', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1813 0416 1202 2021', '72', 'F', 'W', '501', 'SINGLE FAMILY DWELLING', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '8400    TERHUNE                      AV', '', '34.2231', '-118.3644']]\n",
      "Time taken: 23.51 seconds"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "#sc = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"RDD query 1 execution\") \\\n",
    "#    .getOrCreate() \\\n",
    "#    .sparkContext\n",
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loading crime data from the 2 available datasets (2010-2019, 2020-present)\n",
    "crime_data_2010_2019 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "crime_data_2020_present = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "\n",
    "# Extract the header row\n",
    "header = crime_data_2010_2019.first()\n",
    "\n",
    "# Get the indices of the columns to keep\n",
    "dr_no_index = header.index(\"DR_NO\")\n",
    "vict_age_index = header.index(\"Vict Age\")\n",
    "crm_cd_desc_index = header.index(\"Crm Cd Desc\")\n",
    "\n",
    "# Select only the required columns for each dataset\n",
    "selected_crime_data_2010_2019 = crime_data_2010_2019.map(\n",
    "    lambda row: (row[dr_no_index], row[vict_age_index], row[crm_cd_desc_index])\n",
    ")\n",
    "selected_crime_data_2020_present = crime_data_2020_present.map(\n",
    "    lambda row: (row[dr_no_index], row[vict_age_index], row[crm_cd_desc_index])\n",
    ")\n",
    "\n",
    "# Keep the rows which contain the term \"aggravated assault\" on the Crm Cd Desc column\n",
    "filtered_crime_data_2010_2019 = crime_data_2010_2019.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[crm_cd_desc_index])\n",
    "filtered_crime_data_2020_present = crime_data_2020_present.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[crm_cd_desc_index])\n",
    "\n",
    "# Merge the filtered data\n",
    "merged_data = filtered_crime_data_2010_2019.union(filtered_crime_data_2020_present)\n",
    "\n",
    "# Split into age groups then sort by descending age \n",
    "children = merged_data.filter(lambda row: int(row[vict_age_index]) < 18) \\\n",
    "                .sortBy(lambda x: x[1], ascending=False)\n",
    "young_adults = merged_data.filter(lambda row: int(row[vict_age_index]) >= 18 and int(row[vict_age_index]) <= 24) \\\n",
    "                .sortBy(lambda x: x[1], ascending=False)\n",
    "adults = merged_data.filter(lambda row: int(row[vict_age_index]) >= 25 and int(row[vict_age_index]) <= 64) \\\n",
    "                .sortBy(lambda x: x[1], ascending=False)\n",
    "elders = merged_data.filter(lambda row: int(row[vict_age_index]) > 64) \\\n",
    "                .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Test results\n",
    "print(children.take(1))\n",
    "print(young_adults.take(1))\n",
    "print(adults.take(1))\n",
    "print(elders.take(1))\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2089801c-f48c-4e4b-8742-3bf9706c2ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 4\n",
      "+---------+--------------------+--------+\n",
      "|    DR_NO|         Crm Cd Desc|Vict Age|\n",
      "+---------+--------------------+--------+\n",
      "|121800853|CHILD ABUSE (PHYS...|       9|\n",
      "|231314578|INTIMATE PARTNER ...|       9|\n",
      "|220114625|CHILD ABUSE (PHYS...|       9|\n",
      "|230321104|ASSAULT WITH DEAD...|       9|\n",
      "|231917406|CHILD ABUSE (PHYS...|       9|\n",
      "+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+--------+\n",
      "|    DR_NO|         Crm Cd Desc|Vict Age|\n",
      "+---------+--------------------+--------+\n",
      "|190207473|ASSAULT WITH DEAD...|      24|\n",
      "|220616673|ASSAULT WITH DEAD...|      24|\n",
      "|231408338|INTIMATE PARTNER ...|      24|\n",
      "|220610361|ASSAULT WITH DEAD...|      24|\n",
      "|240705661|ASSAULT WITH DEAD...|      24|\n",
      "+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+--------+\n",
      "|    DR_NO|         Crm Cd Desc|Vict Age|\n",
      "+---------+--------------------+--------+\n",
      "|191810607|ASSAULT WITH DEAD...|      64|\n",
      "|231819319|ASSAULT WITH DEAD...|      64|\n",
      "|221308094|ASSAULT WITH DEAD...|      64|\n",
      "|232010007|ASSAULT WITH DEAD...|      64|\n",
      "|240908381|ASSAULT WITH DEAD...|      64|\n",
      "+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+--------+\n",
      "|    DR_NO|         Crm Cd Desc|Vict Age|\n",
      "+---------+--------------------+--------+\n",
      "|191221126|ASSAULT WITH DEAD...|      99|\n",
      "|221816955|ASSAULT WITH DEAD...|      99|\n",
      "|221607836|INTIMATE PARTNER ...|      99|\n",
      "|201207406|ASSAULT WITH DEAD...|      99|\n",
      "|221309255|ASSAULT WITH DEAD...|      99|\n",
      "+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time taken: 11.19 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (8)\n",
      "+- Sort (7)\n",
      "   +- Exchange (6)\n",
      "      +- Union (5)\n",
      "         :- Filter (2)\n",
      "         :  +- Scan csv  (1)\n",
      "         +- Filter (4)\n",
      "            +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [DR_NO#29, Crm Cd Desc#38, Vict Age#40]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(Crm Cd Desc), IsNotNull(Vict Age)]\n",
      "ReadSchema: struct<DR_NO:string,Crm Cd Desc:string,Vict Age:string>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [DR_NO#29, Crm Cd Desc#38, Vict Age#40]\n",
      "Condition : (((isnotnull(Crm Cd Desc#38) AND isnotnull(Vict Age#40)) AND RLIKE(Crm Cd Desc#38, AGGRAVATED ASSAULT)) AND (cast(Vict Age#40 as int) < 18))\n",
      "\n",
      "(3) Scan csv \n",
      "Output [3]: [DR_NO#107, Crm Cd Desc#116, Vict Age#118]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(Crm Cd Desc), IsNotNull(Vict Age)]\n",
      "ReadSchema: struct<DR_NO:string,Crm Cd Desc:string,Vict Age:string>\n",
      "\n",
      "(4) Filter\n",
      "Input [3]: [DR_NO#107, Crm Cd Desc#116, Vict Age#118]\n",
      "Condition : (((isnotnull(Crm Cd Desc#116) AND isnotnull(Vict Age#118)) AND RLIKE(Crm Cd Desc#116, AGGRAVATED ASSAULT)) AND (cast(Vict Age#118 as int) < 18))\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) Exchange\n",
      "Input [3]: [DR_NO#29, Crm Cd Desc#38, Vict Age#40]\n",
      "Arguments: rangepartitioning(Vict Age#40 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=196]\n",
      "\n",
      "(7) Sort\n",
      "Input [3]: [DR_NO#29, Crm Cd Desc#38, Vict Age#40]\n",
      "Arguments: [Vict Age#40 DESC NULLS LAST], true, 0\n",
      "\n",
      "(8) AdaptiveSparkPlan\n",
      "Output [3]: [DR_NO#29, Crm Cd Desc#38, Vict Age#40]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Spark Dataframe Code \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql.functions import col, desc\n",
    "import time\n",
    "\n",
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"DF query 1 execution\") \\\n",
    "#    .getOrCreate() \\\n",
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Loading crime data from the 2 available datasets (2010-2019, 2020-present)\n",
    "# We select only the columns needed for this query which are DR_NO (id), Vict Age, Crm Cd Desc \n",
    "crime_data_2010_2019_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"Crm Cd Desc\", \"Vict Age\")\n",
    "crime_data_2020_present_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"Crm Cd Desc\", \"Vict Age\")\n",
    "\n",
    "# Filter rows where Crime Cd Desc contains the term \"AGGRAVATED ASSAULT\"\n",
    "filtered_crime_data_2010_2019_df = crime_data_2010_2019_df.filter(col(\"Crm Cd Desc\").rlike(\"AGGRAVATED ASSAULT\"))\n",
    "filtered_crime_data_2020_present_df = crime_data_2020_present_df.filter(col(\"Crm Cd Desc\").rlike(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Merge Results\n",
    "merged_df = filtered_crime_data_2010_2019_df.union(filtered_crime_data_2020_present_df)\n",
    "\n",
    "# Filtering and Sorting\n",
    "children = merged_df \\\n",
    "            .filter(col(\"Vict Age\").cast('int') < 18) \\\n",
    "            .sort(desc(\"Vict Age\"))\n",
    "young_adults = merged_df \\\n",
    "            .filter((col(\"Vict Age\").cast('int') >= 18) & (col(\"Vict Age\").cast('int') <= 24)) \\\n",
    "            .sort(desc(\"Vict Age\"))\n",
    "adults = merged_df \\\n",
    "            .filter((col(\"Vict Age\").cast('int') > 24) & (col(\"Vict Age\").cast('int') <= 64)) \\\n",
    "            .sort(desc(\"Vict Age\"))\n",
    "elders = merged_df \\\n",
    "            .filter(col(\"Vict Age\").cast('int') > 64) \\\n",
    "            .sort(desc(\"Vict Age\"))\n",
    "\n",
    "\n",
    "# Test results\n",
    "children.show(5)\n",
    "young_adults.show(5)\n",
    "adults.show(5)\n",
    "elders.show(5)\n",
    "\n",
    "end_time_df = time.time()\n",
    "elapsed_time = end_time_df - start_time_df\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "children.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f6c8c",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "852fbc96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+---+\n",
      "|year|   precinct|  closed_case_rate|  #|\n",
      "+----+-----------+------------------+---+\n",
      "|2010|    Rampart| 32.84713448949121|  1|\n",
      "|2010|    Olympic|31.515289821999087|  2|\n",
      "|2010|     Harbor| 29.36028339237341|  3|\n",
      "|2011|    Olympic|35.040060090135206|  1|\n",
      "|2011|    Rampart|  32.4964471814306|  2|\n",
      "|2011|     Harbor| 28.51336246316431|  3|\n",
      "|2012|    Olympic| 34.29708533302119|  1|\n",
      "|2012|    Rampart| 32.46000463714352|  2|\n",
      "|2012|     Harbor|29.509585848956675|  3|\n",
      "|2013|    Olympic| 33.58217940999398|  1|\n",
      "|2013|    Rampart|  32.1060382916053|  2|\n",
      "|2013|     Harbor|29.727164887307232|  3|\n",
      "|2014|   Van Nuys|  32.0215235281705|  1|\n",
      "|2014|West Valley| 31.49754809505847|  2|\n",
      "|2014|    Mission|31.224939855653567|  3|\n",
      "|2015|   Van Nuys|32.265140677157845|  1|\n",
      "|2015|    Mission|30.463762673676303|  2|\n",
      "|2015|   Foothill|30.353001803658852|  3|\n",
      "|2016|   Van Nuys|32.194518462124094|  1|\n",
      "|2016|West Valley| 31.40146437042384|  2|\n",
      "+----+-----------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total DataFrame API Execution Time: 15.60 seconds"
     ]
    }
   ],
   "source": [
    "# Question 2, part a) (δηλαδή implementation του 2ου query)\n",
    "# DataFrame\n",
    "# part 1 πρώτα, μην ξεχασω τη when κατω todo: remove this\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, desc, rank, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import BooleanType\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DF query 2 execution\").getOrCreate()\n",
    "\n",
    "# Define paths to your dataset files\n",
    "csv_path_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_path_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Load the datasets and combine them\n",
    "crime_data_df = spark.read.format(\"csv\").options(header=\"true\").load(csv_path_2010_2019).union(\n",
    "    spark.read.format(\"csv\").options(header=\"true\").load(csv_path_2020_present)\n",
    ")\n",
    "\n",
    "# Start overall execution timer\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Define mapping for CaseStatus codes\n",
    "status_mapping = {\n",
    "    \"IC\": False,  # Investigation Continued (not closed)\n",
    "    \"UNK\": False,  # Unknown (not closed)\n",
    "}\n",
    "\n",
    "# Define UDF to determine if a case is closed\n",
    "is_closed_udf = udf(lambda status: status_mapping.get(status, True), BooleanType())\n",
    "\n",
    "#print(f\"Step 1 - Filtering rows: {end_step1 - start_step1:.2f} seconds\")\n",
    "\n",
    "# Extract Year and keep relevant columns\n",
    "start_step2 = time.time()\n",
    "\n",
    "#select ωστε το output να ειναι ομοιο με τον Πινακα 2: Υποδειγμα αποτελεσματος Query 2\n",
    "crime_data_filtered = crime_data_df.select(\n",
    "    col(\"DATE OCC\").substr(7, 4).alias(\"Year\"),\n",
    "    col(\"AREA NAME\").alias(\"PoliceStation\"),\n",
    "    col(\"Status\").alias(\"CaseStatus\")\n",
    ").filter(\n",
    "    col(\"Year\").isNotNull() & col(\"PoliceStation\").isNotNull() & col(\"CaseStatus\").isNotNull()\n",
    ").withColumn(\n",
    "    \"IsClosed\", is_closed_udf(col(\"CaseStatus\"))  # Add IsClosed column\n",
    ")\n",
    "end_step2 = time.time()\n",
    "#print(f\"Step 2 - Extracting and filtering columns: {end_step2 - start_step2:.2f} seconds\")\n",
    "\n",
    "# Calculate closed case rate\n",
    "start_step3 = time.time()\n",
    "closed_case_rate = crime_data_filtered.groupBy(\"Year\", \"PoliceStation\").agg(\n",
    "    count(when(col(\"IsClosed\") == True, True)).alias(\"ClosedCases\"),\n",
    "    count(\"*\").alias(\"TotalCases\")\n",
    ").withColumn(\n",
    "    \"closedCaseRate\", (col(\"ClosedCases\") / col(\"TotalCases\")) * 100\n",
    ")\n",
    "end_step3 = time.time()\n",
    "#print(f\"Step 3 - Calculating closed case rate: {end_step3 - start_step3:.2f} seconds\")\n",
    "\n",
    "# Rank police stations by closed case rate\n",
    "start_step4 = time.time()\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"ClosedCaseRate\"))\n",
    "ranked_stations = closed_case_rate.withColumn(\"Rank\", rank().over(window_spec))\n",
    "end_step4 = time.time()\n",
    "#print(f\"Step 4 - Ranking police stations: {end_step4 - start_step4:.2f} seconds\")\n",
    "\n",
    "# Filter top 3 stations per year\n",
    "start_step5 = time.time()\n",
    "top_3_stations_df = ranked_stations.filter(col(\"Rank\") <= 3).orderBy(\"Year\", \"Rank\")\n",
    "end_step5 = time.time()\n",
    "#print(f\"Step 5 - Filtering top 3 stations: {end_step5 - start_step5:.2f} seconds\")\n",
    "\n",
    "# Show results\n",
    "top_3_stations_df.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"PoliceStation\").alias(\"precinct\"),\n",
    "    col(\"ClosedCaseRate\").alias(\"closed_case_rate\"),\n",
    "    col(\"Rank\").alias(\"#\")\n",
    ").show()\n",
    "\n",
    "# End overall execution timer\n",
    "end_time_df = time.time()\n",
    "print(f\"Total DataFrame API Execution Time: {end_time_df - start_time_df:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "771cbd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------------+---+\n",
      "|year|   precinct| closed_case_rate|  #|\n",
      "+----+-----------+-----------------+---+\n",
      "|2010|    Rampart|32.84713448949121|  1|\n",
      "|2010|    Olympic|31.51528982199909|  2|\n",
      "|2010|     Harbor|29.36028339237341|  3|\n",
      "|2011|    Olympic|35.04006009013520|  1|\n",
      "|2011|    Rampart|32.49644718143060|  2|\n",
      "|2011|     Harbor|28.51336246316431|  3|\n",
      "|2012|    Olympic|34.29708533302119|  1|\n",
      "|2012|    Rampart|32.46000463714352|  2|\n",
      "|2012|     Harbor|29.50958584895668|  3|\n",
      "|2013|    Olympic|33.58217940999398|  1|\n",
      "|2013|    Rampart|32.10603829160530|  2|\n",
      "|2013|     Harbor|29.72716488730724|  3|\n",
      "|2014|   Van Nuys|32.02152352817050|  1|\n",
      "|2014|West Valley|31.49754809505847|  2|\n",
      "|2014|    Mission|31.22493985565357|  3|\n",
      "|2015|   Van Nuys|32.26514067715784|  1|\n",
      "|2015|    Mission|30.46376267367630|  2|\n",
      "|2015|   Foothill|30.35300180365885|  3|\n",
      "|2016|   Van Nuys|32.19451846212410|  1|\n",
      "|2016|West Valley|31.40146437042384|  2|\n",
      "+----+-----------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "SQL API Execution Time: 5.76 seconds"
     ]
    }
   ],
   "source": [
    "#same, just SQL API implementation\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Start time for SQL API\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data_df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query for Query 2\n",
    "sql_query = \"\"\"\n",
    "    WITH ClosedCases AS (\n",
    "        SELECT \n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS Year,\n",
    "            `AREA NAME` AS PoliceStation,\n",
    "            COUNT(CASE WHEN `Status` NOT IN ('IC', 'UNK') THEN 1 END) AS ClosedCases,\n",
    "            COUNT(*) AS TotalCases,\n",
    "            (COUNT(CASE WHEN `Status` NOT IN ('IC', 'UNK') THEN 1 END) * 100.0 / COUNT(*)) AS ClosedCaseRate\n",
    "        FROM crime_data\n",
    "        WHERE `DATE OCC` IS NOT NULL\n",
    "          AND `AREA NAME` IS NOT NULL\n",
    "          AND `Status` IS NOT NULL\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    RankedStations AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            PoliceStation,\n",
    "            ClosedCaseRate,\n",
    "            RANK() OVER (PARTITION BY Year ORDER BY ClosedCaseRate DESC) AS Rank\n",
    "        FROM ClosedCases\n",
    "    )\n",
    "    SELECT \n",
    "        Year AS year,\n",
    "        PoliceStation AS precinct,\n",
    "        ClosedCaseRate AS closed_case_rate,\n",
    "        Rank AS `#`\n",
    "    FROM RankedStations\n",
    "    WHERE Rank <= 3\n",
    "    ORDER BY Year, Rank\n",
    "\"\"\"\n",
    "\n",
    "# Execute SQL query\n",
    "top_3_stations_sql = spark.sql(sql_query)\n",
    "\n",
    "# Show results\n",
    "top_3_stations_sql.show()\n",
    "\n",
    "# End time for SQL API\n",
    "end_time_sql = time.time()\n",
    "\n",
    "# Print execution time\n",
    "print(f\"SQL API Execution Time: {end_time_sql - start_time_sql:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49d15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22166aa4d0514c9f9626bb37d4c18dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#part b of question 2 (2b)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, desc, rank, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import BooleanType\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Query2 Parquet vs CSV Comparison\").getOrCreate()\n",
    "\n",
    "# Define paths\n",
    "csv_path_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_path_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "parquet_path = \"s3://groups-bucket-dblab-905418150721/group31/main_dataset_unique.parquet\"\n",
    "\n",
    "# Step 1: Save dataset as a single Parquet file\n",
    "#print(\"Saving dataset as a unique Parquet file...\")\n",
    "start_parquet_save = time.time()\n",
    "\n",
    "# Combine datasets and save as a single Parquet file\n",
    "crime_data_df = spark.read.format(\"csv\").options(header=\"true\").load(csv_path_2010_2019).union(\n",
    "    spark.read.format(\"csv\").options(header=\"true\").load(csv_path_2020_present)\n",
    ")\n",
    "\n",
    "crime_data_df.coalesce(1).write.mode(\"overwrite\").parquet(parquet_path)\n",
    "end_parquet_save = time.time()\n",
    "#print(f\"Parquet Save Time: {end_parquet_save - start_parquet_save:.2f} seconds\")\n",
    "#todo : uncomment εαν χρειαζονται αναλυτικοι χρονοι (ζηταται μονο το execution time οποτε μαλλον οχι)\n",
    "\n",
    "# Step 2: Query 2 on CSV\n",
    "#print(\"Executing Query 2 on CSV format...\")\n",
    "start_csv_query = time.time()\n",
    "\n",
    "# Load CSV files and apply Query 2 logic\n",
    "crime_data_csv = spark.read.format(\"csv\") \\\n",
    "    .options(header=\"true\") \\\n",
    "    .load(csv_path_2010_2019) \\\n",
    "    .union(\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(header=\"true\")\n",
    "        .load(csv_path_2020_present)\n",
    "    )\n",
    "\n",
    "# Define the status mapping for closed cases\n",
    "status_mapping = {\"IC\": False, \"UNK\": False}\n",
    "is_closed_udf = udf(lambda status: status_mapping.get(status, True), BooleanType())\n",
    "\n",
    "crime_data_csv_filtered = crime_data_csv.select(\n",
    "    col(\"DATE OCC\").substr(7, 4).alias(\"Year\"),\n",
    "    col(\"AREA NAME\").alias(\"PoliceStation\"),\n",
    "    col(\"Status\").alias(\"CaseStatus\")\n",
    ").filter(\n",
    "    col(\"Year\").isNotNull() & col(\"PoliceStation\").isNotNull() & col(\"CaseStatus\").isNotNull()\n",
    ").withColumn(\n",
    "    \"IsClosed\", is_closed_udf(col(\"CaseStatus\"))\n",
    ")\n",
    "\n",
    "closed_case_rate = crime_data_csv_filtered.groupBy(\"Year\", \"PoliceStation\").agg(\n",
    "    count(when(col(\"IsClosed\") == True, True)).alias(\"ClosedCases\"),\n",
    "    count(\"*\").alias(\"TotalCases\")\n",
    ").withColumn(\n",
    "    \"ClosedCaseRate\", (col(\"ClosedCases\") / col(\"TotalCases\")) * 100\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"ClosedCaseRate\"))\n",
    "top_3_stations_csv = closed_case_rate.withColumn(\"Rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"Rank\") <= 3).orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "# Select only the desired columns\n",
    "top_3_stations_csv.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"PoliceStation\").alias(\"precinct\"),\n",
    "    col(\"ClosedCaseRate\").alias(\"closed_case_rate\"),\n",
    "    col(\"Rank\").alias(\"#\")\n",
    ").show()\n",
    "\n",
    "end_csv_query = time.time()\n",
    "print(f\"CSV Query Execution Time: {end_csv_query - start_csv_query:.2f} seconds\")\n",
    "\n",
    "# Step 3: Query 2 on Parquet\n",
    "#print(\"Executing Query 2 on Parquet format...\")\n",
    "start_parquet_query = time.time()\n",
    "\n",
    "# Load Parquet file and apply Query 2 logic\n",
    "crime_data_parquet = spark.read.format(\"parquet\").load(parquet_path)\n",
    "\n",
    "crime_data_parquet_filtered = crime_data_parquet.select(\n",
    "    col(\"DATE OCC\").substr(7, 4).alias(\"Year\"),\n",
    "    col(\"AREA NAME\").alias(\"PoliceStation\"),\n",
    "    col(\"Status\").alias(\"CaseStatus\")\n",
    ").filter(\n",
    "    col(\"Year\").isNotNull() & col(\"PoliceStation\").isNotNull() & col(\"CaseStatus\").isNotNull()\n",
    ").withColumn(\n",
    "    \"IsClosed\", is_closed_udf(col(\"CaseStatus\"))\n",
    ")\n",
    "\n",
    "closed_case_rate_parquet = crime_data_parquet_filtered.groupBy(\"Year\", \"PoliceStation\").agg(\n",
    "    count(when(col(\"IsClosed\") == True, True)).alias(\"ClosedCases\"),\n",
    "    count(\"*\").alias(\"TotalCases\")\n",
    ").withColumn(\n",
    "    \"ClosedCaseRate\", (col(\"ClosedCases\") / col(\"TotalCases\")) * 100\n",
    ")\n",
    "\n",
    "top_3_stations_parquet = closed_case_rate_parquet.withColumn(\"Rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"Rank\") <= 3).orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "# Select only the desired columns\n",
    "top_3_stations_parquet.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"PoliceStation\").alias(\"precinct\"),\n",
    "    col(\"ClosedCaseRate\").alias(\"closed_case_rate\"),\n",
    "    col(\"Rank\").alias(\"#\")\n",
    ").show()\n",
    "\n",
    "end_parquet_query = time.time()\n",
    "print(f\"Parquet Query Execution Time: {end_parquet_query - start_parquet_query:.2f} seconds\")\n",
    "\n",
    "print('\\n'*3)\n",
    "#λιγος κενος χωρος να φαινεται η συγκριση\n",
    "\n",
    "# Step 4: Summary of results\n",
    "print(\"CSV Vs Parquet Execution Times:\")\n",
    "print(f\"CSV Query Execution Time: {end_csv_query - start_csv_query:.2f} seconds\")\n",
    "print(f\"Parquet Query Execution Time: {end_parquet_query - start_parquet_query:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2475ba1-0037-4814-8a5b-af6f4aa7ba21",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b681db6-e833-4352-8ddf-621eccd2ba75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip Codes join broadcast: 13.92885160446167\n",
      "+------------------+-----------+---------------+--------------------------------+---------------------+\n",
      "|              COMM|Comm Houses|Comm Population|Comm Median Income per Household|Avg Income per Person|\n",
      "+------------------+-----------+---------------+--------------------------------+---------------------+\n",
      "|    Toluca Terrace|        541|           1301|                         48499.0|   20167.531898539586|\n",
      "|      Elysian Park|       1993|           5267|                35151.9801980198|   13301.290399592457|\n",
      "|          Longwood|       1474|           4210|                         38330.0|   13420.052256532066|\n",
      "|     Green Meadows|       5204|          19821|              30573.460674157304|   8027.0566242023415|\n",
      "|  Cadillac-Corning|       2215|           6665|                         62425.2|    20745.95918979745|\n",
      "|          Mid-city|       6692|          14339|                         46571.0|    21734.64899923286|\n",
      "|   Lincoln Heights|       9316|          31144|               37990.15120274914|    11363.86618946863|\n",
      "|          Van Nuys|      29170|          86019|              43827.914666666664|    14862.53351964876|\n",
      "|    Gramercy Place|       3941|          10361|                         39269.0|   14936.698098639128|\n",
      "| Faircrest Heights|       1356|           3443|               52901.33802816901|   20834.799409293402|\n",
      "|     Boyle Heights|      21548|          82536|               32785.99374021909|     8559.56907427354|\n",
      "|  Lafayette Square|       1563|           4358|              46328.617647058825|    16615.79380044813|\n",
      "|     Granada Hills|      18855|          55172|               78171.74825174826|   26715.151041954494|\n",
      "|       North Hills|      16208|          56375|               56409.98540145985|   16218.058419279136|\n",
      "|        Northridge|      23328|          62037|              60909.710144927536|    22904.10107292212|\n",
      "|   Wilshire Center|      19835|          47295|               38425.48837209302|   16115.224904545199|\n",
      "|    Jefferson Park|       2306|           7612|                         33864.0|   10258.852338413031|\n",
      "|    Vermont Square|       1953|           7045|              30028.615384615383|    8324.469247147457|\n",
      "|Cloverdale/Cochran|       4984|          14032|               41931.10924369748|   14893.432758736333|\n",
      "|   Adams-Normandie|       2424|           7842|              29397.636363636364|     9086.95110245531|\n",
      "+------------------+-----------+---------------+--------------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+---------------+------------------------------+-----------------+\n",
      "|             COMM|Comm Population|Total Number of Crimes in Comm|Crimes per Person|\n",
      "+-----------------+---------------+------------------------------+-----------------+\n",
      "|        Hyde Park|          27943|                         30263|     1.0830261604|\n",
      "|     View Heights|           3257|                          2259|     0.6935830519|\n",
      "|      Westchester|          48017|                         57124|     1.1896619947|\n",
      "|       Exposition|           3238|                          4379|     1.3523780111|\n",
      "|    Baldwin Hills|          28637|                         34194|      1.194049656|\n",
      "|Crenshaw District|          13158|                         13297|     1.0105639155|\n",
      "|        Brentwood|          29301|                         14756|     0.5036005597|\n",
      "|         Westwood|          51529|                         20698|     0.4016767257|\n",
      "|        Mar Vista|          39126|                         19434|     0.4967029597|\n",
      "| West Los Angeles|          34165|                         28397|     0.8311722523|\n",
      "|         Westlake|          54638|                         54794|     1.0028551558|\n",
      "|       West Adams|          26543|                         20771|     0.7825415364|\n",
      "|  University Park|          24842|                         25793|     1.0382819419|\n",
      "|       Pico-Union|          39808|                         31109|     0.7814760852|\n",
      "|        Hollywood|          62412|                         96810|     1.5511440108|\n",
      "|   Little Armenia|           7473|                          6467|      0.865382042|\n",
      "|  Hollywood Hills|          27895|                         22669|     0.8126545976|\n",
      "|        Thai Town|           9350|                          4961|     0.5305882353|\n",
      "|        Los Feliz|          20558|                         18659|     0.9076272011|\n",
      "|     Sherman Oaks|          81443|                         64370|     0.7903687241|\n",
      "+-----------------+---------------+------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Shuffle Replicate NL: 19.08557653427124\n",
      "+-------------------+---------------+------------------------------+-----------------+-----------+--------------------------------+---------------------+\n",
      "|               COMM|Comm Population|Total Number of Crimes in Comm|Crimes per Person|Comm Houses|Comm Median Income per Household|Avg Income per Person|\n",
      "+-------------------+---------------+------------------------------+-----------------+-----------+--------------------------------+---------------------+\n",
      "|  Pacific Palisades|          20643|                          9307|     0.4508550114|       9048|              160905.47810218978|     70526.2203104497|\n",
      "|      Beverly Crest|          12191|                          4527|     0.3713395128|       5393|              150356.19753086418|    66513.90150799365|\n",
      "|   Marina Peninsula|           4337|                          2656|     0.6124048882|       2815|                        100507.0|    65235.69402813004|\n",
      "|Palisades Highlands|           3833|                           788|     0.2055830942|       1545|               161380.3488372093|    65048.95354904471|\n",
      "|            Bel Air|           8261|                          3532|     0.4275511439|       3456|              151212.57777777777|    63259.97685510228|\n",
      "|  Mandeville Canyon|           3233|                           848|      0.262295082|       1342|              148023.85714285713|    61443.86522911051|\n",
      "|          Brentwood|          29301|                         14756|     0.5036005597|      15437|              115208.67279411765|   60696.777650004915|\n",
      "|            Carthay|          13165|                         12273|     0.9322445879|       8303|                79726.8025477707|   50282.692104378286|\n",
      "|             Venice|          32625|                         40942|     1.2549272031|      18753|               81028.73935264055|    46575.69192582585|\n",
      "|       Century City|          11890|                         10176|     0.8558452481|       6619|               82106.45161290323|    45707.53601562712|\n",
      "|      Playa Del Rey|           3158|                          2454|     0.7770740975|       1752|                         82055.0|      45522.596580114|\n",
      "|        Playa Vista|           8926|                          7281|     0.8157069236|       4808|               82561.97321428571|   44472.100292884345|\n",
      "|    Hollywood Hills|          27895|                         22669|     0.8126545976|      17180|               70977.34532374101|   43713.597155829746|\n",
      "|        Studio City|          20703|                         19245|     0.9295754239|      10740|               81359.23144104803|    42206.35394275496|\n",
      "|   West Los Angeles|          34165|                         28397|     0.8311722523|      18977|               73783.34364261168|    40983.06782689424|\n",
      "|      South Carthay|          10093|                          7299|     0.7231744774|       5519|                         72497.0|   39642.419795898146|\n",
      "|             Encino|          42349|                         29803|     0.7037474321|      18961|                88326.6334231806|    39546.65508835928|\n",
      "|       Miracle Mile|          16057|                         14163|     0.8820452139|       9694|               64569.10588235294|    38981.93388699816|\n",
      "|        Rancho Park|           6295|                          7902|     1.2552819698|       2794|                         87283.0|   38740.063860206516|\n",
      "|       Sherman Oaks|          81443|                         64370|     0.7903687241|      41461|               73613.64246575342|     37475.2309010302|\n",
      "+-------------------+---------------+------------------------------+-----------------+-----------+--------------------------------+---------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum, avg, countDistinct, regexp_replace, count, desc, round\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 3 implementation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Load la income per house dataframe\n",
    "la_income_2015_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\") \\\n",
    "\n",
    "# We will join the 2010 data with the 2015 data based on the zip code of the areas\n",
    "# To do so the zipcode columns must have the same data type\n",
    "# So we change the datatype of flattened_df accordingly\n",
    "# We also filter out null values in the columns of interest and keep the rows corresponding to LA city areas\n",
    "flattened_df = flattened_df \\\n",
    "            .withColumn(\"ZCTA10\", flattened_df[\"ZCTA10\"].cast(\"int\")) \\\n",
    "            .filter(col(\"COMM\").isNotNull() & col(\"POP_2010\").isNotNull() & col(\"HOUSING10\").isNotNull()) \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Join on Zip Code\n",
    "joined_df = flattened_df.join(la_income_2015_df.hint(\"broadcast\"), flattened_df[\"ZCTA10\"] == la_income_2015_df[\"Zip Code\"], how=\"inner\")\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df.collect()\n",
    "print(f\"Zip Codes join broadcast: {time.time()-start_time}\")\n",
    "\n",
    "# Convert the Estimated Median Income column to integer\n",
    "# By removing \"$\" and \",\"\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Average Income per Person = (Estimated Median Income)*(Number of Houses)/(Community Population)\n",
    "average_income_per_person_df = joined_df \\\n",
    "        .groupBy(\"COMM\") \\\n",
    "        .agg(\n",
    "            sum(\"HOUSING10\").alias(\"Comm Houses\"),\n",
    "            sum(\"POP_2010\").alias(\"Comm Population\"),\n",
    "            avg(\"Median Income\").alias(\"Comm Median Income per Household\")\n",
    "        ) \\\n",
    "        .withColumn(\"Avg Income per Person\", col(\"Comm Median Income per Household\")*col(\"Comm Houses\")/col(\"Comm Population\")) \\\n",
    "\n",
    "average_income_per_person_df.show()\n",
    "\n",
    "# Load and join crime datasets\n",
    "crime_data_2010_2019_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"LAT\", \"LON\")   # No other columns are needed\n",
    "crime_data_2020_present_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"LAT\", \"LON\")\n",
    "\n",
    "crime_data_df = crime_data_2010_2019_df.union(crime_data_2020_present_df)\n",
    "\n",
    "# Create geometry column for the crime data \n",
    "crime_data_df = crime_data_df.withColumn(\"geometry\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# We will pre-calculate the community population that we need for this query \n",
    "# Because it will be harder after the join\n",
    "# Define a window partitioned by \"COMM\"\n",
    "community_window = Window.partitionBy(\"COMM\")\n",
    "\n",
    "# Calculate the total population per community and add it as a new column\n",
    "df_with_comm_population = flattened_df.withColumn(\n",
    "    \"Comm Population\",\n",
    "    sum(\"POP_2010\").over(community_window)\n",
    ")\n",
    "\n",
    "# Join based on geometry\n",
    "joined_df_1 = crime_data_df \\\n",
    "        .join(df_with_comm_population, ST_Within(crime_data_df.geometry, df_with_comm_population.geometry), \"inner\")\n",
    "\n",
    "#start_time = time.time()\n",
    "#joined_df_1.collect()\n",
    "#print(f\"geometry join broadcast:{time.time()-start_time}\")\n",
    "\n",
    "# Crimes per person = Total Crimes/Population\n",
    "crimes_per_person_df = joined_df_1 \\\n",
    "            .groupBy(\"COMM\", \"Comm Population\") \\\n",
    "            .agg(\n",
    "                count(\"DR_NO\").alias(\"Total Number of Crimes in Comm\"),\n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"Crimes per Person\", \n",
    "                round(col(\"Total Number of Crimes in Comm\")/col(\"Comm Population\"), 10)\n",
    "            )\n",
    "\n",
    "crimes_per_person_df.show()\n",
    "\n",
    "info_per_person_df = crimes_per_person_df.hint(\"shuffle_replicate_nl\").join(average_income_per_person_df, on=[\"COMM\", \"Comm Population\"], how=\"inner\").orderBy(desc(\"Avg Income per Person\"))\n",
    "\n",
    "# Force the execution of the join\n",
    "# And measure performance\n",
    "start_time = time.time()\n",
    "info_per_person_df.collect()\n",
    "print(f\"Shuffle Replicate NL: {time.time()-start_time}\")\n",
    "\n",
    "info_per_person_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0528fec3-d2eb-4780-8f9b-23d0572a51f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- BroadcastHashJoin Inner BuildRight (9)\n",
      "      :- Project (5)\n",
      "      :  +- Filter (4)\n",
      "      :     +- Generate (3)\n",
      "      :        +- Filter (2)\n",
      "      :           +- Scan geojson  (1)\n",
      "      +- BroadcastExchange (8)\n",
      "         +- Filter (7)\n",
      "            +- Scan csv  (6)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#6814]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#6814]\n",
      "Condition : ((size(features#6814, true) > 0) AND isnotnull(features#6814))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#6814]\n",
      "Arguments: explode(features#6814), false, [features#6822]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#6822]\n",
      "Condition : ((isnotnull(features#6822.properties.CITY) AND (((isnotnull(features#6822.properties.COMM) AND isnotnull(features#6822.properties.POP_2010)) AND isnotnull(features#6822.properties.HOUSING10)) AND (features#6822.properties.CITY = Los Angeles))) AND isnotnull(cast(features#6822.properties.ZCTA10 as int)))\n",
      "\n",
      "(5) Project\n",
      "Output [26]: [features#6822.properties.BG10 AS BG10#6831, features#6822.properties.BG10FIP10 AS BG10FIP10#6832, features#6822.properties.BG12 AS BG12#6833, features#6822.properties.CB10 AS CB10#6834, features#6822.properties.CEN_FIP13 AS CEN_FIP13#6835, features#6822.properties.CITY AS CITY#6836, features#6822.properties.CITYCOM AS CITYCOM#6837, features#6822.properties.COMM AS COMM#6838, features#6822.properties.CT10 AS CT10#6839, features#6822.properties.CT12 AS CT12#6840, features#6822.properties.CTCB10 AS CTCB10#6841, features#6822.properties.HD_2012 AS HD_2012#6842L, features#6822.properties.HD_NAME AS HD_NAME#6843, features#6822.properties.HOUSING10 AS HOUSING10#6844L, features#6822.properties.LA_FIP10 AS LA_FIP10#6845, features#6822.properties.OBJECTID AS OBJECTID#6846L, features#6822.properties.POP_2010 AS POP_2010#6847L, features#6822.properties.PUMA10 AS PUMA10#6848, features#6822.properties.SPA_2012 AS SPA_2012#6849L, features#6822.properties.SPA_NAME AS SPA_NAME#6850, features#6822.properties.SUP_DIST AS SUP_DIST#6851, features#6822.properties.SUP_LABEL AS SUP_LABEL#6852, features#6822.properties.ShapeSTArea AS ShapeSTArea#6853, features#6822.properties.ShapeSTLength AS ShapeSTLength#6854, cast(features#6822.properties.ZCTA10 as int) AS ZCTA10#6983, features#6822.geometry AS geometry#6825]\n",
      "Input [1]: [features#6822]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [3]: [Zip Code#6977, Community#6978, Estimated Median Income#6979]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [Zip Code#6977, Community#6978, Estimated Median Income#6979]\n",
      "Condition : isnotnull(Zip Code#6977)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [3]: [Zip Code#6977, Community#6978, Estimated Median Income#6979]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [plan_id=4143]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#6983]\n",
      "Right keys [1]: [cast(Zip Code#6977 as int)]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [30]: [BG10#6831, BG10FIP10#6832, BG12#6833, CB10#6834, CEN_FIP13#6835, CITY#6836, CITYCOM#6837, COMM#6838, CT10#6839, CT12#6840, CTCB10#6841, HD_2012#6842L, HD_NAME#6843, HOUSING10#6844L, LA_FIP10#6845, OBJECTID#6846L, POP_2010#6847L, PUMA10#6848, SPA_2012#6849L, SPA_NAME#6850, SUP_DIST#6851, SUP_LABEL#6852, ShapeSTArea#6853, ShapeSTLength#6854, ZCTA10#6983, geometry#6825, Zip Code#6977, Community#6978, Estimated Median Income#6979, cast(regexp_replace(Estimated Median Income#6979, [\\$,], , 1) as int) AS Median Income#7080]\n",
      "Input [29]: [BG10#6831, BG10FIP10#6832, BG12#6833, CB10#6834, CEN_FIP13#6835, CITY#6836, CITYCOM#6837, COMM#6838, CT10#6839, CT12#6840, CTCB10#6841, HD_2012#6842L, HD_NAME#6843, HOUSING10#6844L, LA_FIP10#6845, OBJECTID#6846L, POP_2010#6847L, PUMA10#6848, SPA_2012#6849L, SPA_NAME#6850, SUP_DIST#6851, SUP_LABEL#6852, ShapeSTArea#6853, ShapeSTLength#6854, ZCTA10#6983, geometry#6825, Zip Code#6977, Community#6978, Estimated Median Income#6979]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [30]: [BG10#6831, BG10FIP10#6832, BG12#6833, CB10#6834, CEN_FIP13#6835, CITY#6836, CITYCOM#6837, COMM#6838, CT10#6839, CT12#6840, CTCB10#6841, HD_2012#6842L, HD_NAME#6843, HOUSING10#6844L, LA_FIP10#6845, OBJECTID#6846L, POP_2010#6847L, PUMA10#6848, SPA_2012#6849L, SPA_NAME#6850, SUP_DIST#6851, SUP_LABEL#6852, ShapeSTArea#6853, ShapeSTLength#6854, ZCTA10#6983, geometry#6825, Zip Code#6977, Community#6978, Estimated Median Income#6979, Median Income#7080]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8510d-a6eb-4235-89eb-48d1b1399bd5",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccafac0-84e6-439e-900a-32c57b078eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>35</td><td>application_1738075734771_0036</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0036/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-228.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0036_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>15</td><td>application_1738075734771_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0016_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>18</td><td>application_1738075734771_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0019_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>21</td><td>application_1738075734771_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0022_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>22</td><td>application_1738075734771_0023</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0023/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0023_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>26</td><td>application_1738075734771_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0027_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>30</td><td>application_1738075734771_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0031_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>35</td><td>application_1738075734771_0036</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0036/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-228.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0036_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr><tr><td>36</td><td>application_1738075734771_0037</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868f8971-bc93-45d1-b6d3-90c4d653bf28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 2g\n",
      "Executor Cores: 1\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|894|\n",
      "|               Other|156|\n",
      "|Hispanic/Latin/Me...| 97|\n",
      "|               Black| 55|\n",
      "|             Unknown| 50|\n",
      "|         Other Asian| 31|\n",
      "|American Indian/A...|  1|\n",
      "|             Chinese|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3191|\n",
      "|               Black| 872|\n",
      "|               White| 430|\n",
      "|               Other| 265|\n",
      "|         Other Asian| 140|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  24|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Elapsed time: 94.40970206260681"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum, avg, countDistinct, regexp_replace, count, desc, round\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 4 implementation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# We now follow the same steps as query 3 to recalculate the average income per person\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Load la income per house dataframe\n",
    "la_income_2015_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\") \\\n",
    "\n",
    "# We will join the 2010 data with the 2015 data based on the zip code of the areas\n",
    "# To do so the zipcode columns must have the same data type\n",
    "# So we change the datatype of flattened_df accordingly\n",
    "flattened_df = flattened_df \\\n",
    "        .withColumn(\"ZCTA10\", flattened_df[\"ZCTA10\"].cast(\"int\")) \\\n",
    "        .filter(col(\"COMM\").isNotNull() & col(\"POP_2010\").isNotNull() & col(\"HOUSING10\").isNotNull()) \\\n",
    "        .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Join on Zip Code\n",
    "joined_df = flattened_df.join(la_income_2015_df, flattened_df[\"ZCTA10\"] == la_income_2015_df[\"Zip Code\"], how=\"inner\")\n",
    "\n",
    "# Convert the Estimated Median Income column to integer\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Average Income per Person = (Estimated Median Income)*(Number of Houses)/(Community Population)\n",
    "average_income_per_person_df = joined_df \\\n",
    "        .groupBy(\"COMM\") \\\n",
    "        .agg(\n",
    "            sum(\"HOUSING10\").alias(\"Comm Houses\"),\n",
    "            sum(\"POP_2010\").alias(\"Comm Population\"),\n",
    "            avg(\"Median Income\").alias(\"Comm Median Income per Household\")\n",
    "        ) \\\n",
    "        .withColumn(\"Avg Income per Person\", col(\"Comm Median Income per Household\")*col(\"Comm Houses\")/col(\"Comm Population\")) \\\n",
    "        .orderBy(desc(\"Avg Income per Person\"))\n",
    "\n",
    "# End of query 3 recalculations\n",
    "\n",
    "# Load 2010-2019 crime dataset \n",
    "crime_data_2010_2019_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"DATE OCC\", \"Vict Descent\", \"LON\", \"LAT\")  # No other columns are needed\n",
    "\n",
    "# Keep 2015 data\n",
    "crime_data_filtered_df = crime_data_2010_2019_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "# Create geometry column for the crime data \n",
    "crime_data_filtered_df = crime_data_filtered_df.withColumn(\"geometry\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Find top 3 richest and top 3 poorest areas\n",
    "top_3_richest_areas = average_income_per_person_df.orderBy(desc(\"Avg Income per Person\")).limit(3)\n",
    "top_3_poorest_areas = average_income_per_person_df.orderBy(col(\"Avg Income per Person\")).limit(3)\n",
    "\n",
    "# We need every block from each area to do the geometry join\n",
    "# So we join with the 2010 census blocks dataframe\n",
    "# This is like filtering the census dataframe by only choosing the desired areas\n",
    "richest_areas_blocks_df = flattened_df.join(top_3_richest_areas, on=\"COMM\", how=\"inner\")\n",
    "poorest_areas_blocks_df = flattened_df.join(top_3_poorest_areas, on=\"COMM\", how=\"inner\")\n",
    "\n",
    "# Join based on geometry to find the crimes in the 3 areas of each dataset\n",
    "joined_df_richest = crime_data_filtered_df \\\n",
    "    .join(richest_areas_blocks_df, ST_Within(crime_data_filtered_df.geometry, richest_areas_blocks_df.geometry), \"inner\")\n",
    "joined_df_poorest = crime_data_filtered_df \\\n",
    "    .join(poorest_areas_blocks_df, ST_Within(crime_data_filtered_df.geometry, poorest_areas_blocks_df.geometry), \"inner\")\n",
    "\n",
    "# Load race codes dataframe\n",
    "race_eth_codes_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\") \\\n",
    "                .withColumnRenamed(\"Vict Descent Full\", \"Victim Descent\")\n",
    "\n",
    "# Count the victims per race, then join with the ethnicity data frame to get the full ethnicity names\n",
    "# Select the desired columns and sort in descending order\n",
    "richest_vict_descent_df = joined_df_richest.groupBy(\"Vict Descent\").agg(count(\"*\").alias(\"#\")) \\\n",
    "    .join(race_eth_codes_df, on=\"Vict Descent\", how=\"inner\") \\\n",
    "    .select(\"Victim Descent\", \"#\") \\\n",
    "    .orderBy(desc(\"#\"))\n",
    "poorest_vict_descent_df = joined_df_poorest.groupBy(\"Vict Descent\").agg(count(\"*\").alias(\"#\")) \\\n",
    "    .join(race_eth_codes_df, on=\"Vict Descent\", how=\"inner\") \\\n",
    "    .select(\"Victim Descent\", \"#\") \\\n",
    "    .orderBy(desc(\"#\"))\n",
    "\n",
    "# NOTE: Some Vict Descent values are null\n",
    "richest_vict_descent_df.show()\n",
    "poorest_vict_descent_df.show()\n",
    "\n",
    "print(f\"Elapsed time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99701c55-3ea2-4ae5-932e-cd6d4c26d4d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8651eb-d310-4cdb-b418-e51676a7da19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>44</td><td>application_1738075734771_0045</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0045/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0045_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '4g', 'spark.executor.cores': '2'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>15</td><td>application_1738075734771_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0016_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>18</td><td>application_1738075734771_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0019_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>21</td><td>application_1738075734771_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0022_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>22</td><td>application_1738075734771_0023</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0023/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0023_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>26</td><td>application_1738075734771_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0027_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>30</td><td>application_1738075734771_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0031_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>43</td><td>application_1738075734771_0044</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0044/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0044_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>44</td><td>application_1738075734771_0045</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0045/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0045_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd65d3fd-7a95-4c80-81cd-84b43079919a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 4\n",
      "Executor Memory: 4g\n",
      "Executor Cores: 2\n",
      "+----------------+----------------+------+\n",
      "|        DIVISION|average distance|     #|\n",
      "+----------------+----------------+------+\n",
      "|     77TH STREET|             2.7|206784|\n",
      "|       SOUTHWEST|             2.7|192226|\n",
      "|         PACIFIC|             3.9|170903|\n",
      "|         CENTRAL|             1.1|166698|\n",
      "| NORTH HOLLYWOOD|             2.6|164532|\n",
      "|       SOUTHEAST|             2.1|161051|\n",
      "|       HOLLYWOOD|             1.6|150663|\n",
      "|          NEWTON|             2.1|148757|\n",
      "|         OLYMPIC|             1.8|144962|\n",
      "|         MISSION|             4.7|143600|\n",
      "|       NORTHEAST|             3.8|142732|\n",
      "|        VAN NUYS|             2.4|142194|\n",
      "|         TOPANGA|             3.8|138642|\n",
      "|      DEVONSHIRE|             3.9|137881|\n",
      "|        WILSHIRE|             2.6|136199|\n",
      "|         RAMPART|             1.6|135948|\n",
      "|WEST LOS ANGELES|             3.6|134259|\n",
      "|          HARBOR|             4.1|132911|\n",
      "|     WEST VALLEY|             3.6|131502|\n",
      "|      HOLLENBECK|             2.7|114517|\n",
      "+----------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Elapsed time: 17.99382710456848"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum, avg, countDistinct, regexp_replace, count, desc, round\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#    .appName(\"Query 3 implementation\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and join crime datasets\n",
    "# For some reason AREA is written with a whitespace in 2010-2019 dataset\n",
    "# So we renamed it\n",
    "crime_data_2010_2019_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"AREA \", \"LAT\", \"LON\") \\\n",
    "                .withColumnRenamed(\"AREA \", \"AREA\")\n",
    "crime_data_2020_present_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "                .select(\"DR_NO\", \"AREA\", \"LAT\", \"LON\")\n",
    "\n",
    "crime_data_df = crime_data_2010_2019_df.union(crime_data_2020_present_df)\n",
    "\n",
    "# Create geometry column for crime data after filtering out NULL values\n",
    "crime_data_df = crime_data_df \\\n",
    "                .filter((crime_data_df[\"LON\"] != 0) & (crime_data_df[\"LAT\"] != 0)) \\\n",
    "                .withColumn(\"cd_geometry\", ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .withColumn(\"AREA\", crime_data_df[\"AREA\"].cast(\"int\"))\n",
    "\n",
    "# Load police stations data\n",
    "la_police_stations_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\") \\\n",
    "\n",
    "# Create geomery column for police stations data\n",
    "la_police_stations_df = la_police_stations_df \\\n",
    "                .withColumn(\"pd_geometry\", ST_Point(\"x\", \"y\")) \\\n",
    "                .withColumn(\"PREC\", la_police_stations_df[\"PREC\"].cast(\"int\"))\n",
    "\n",
    "# We needed to cast AREA and PREC to int because single digit numbers have value \"0x\" in crimes dataset\n",
    "# And value \"x\" in police stations dataset\n",
    "\n",
    "# Join the 2 dataframes on the LAPD number attribute  (1-21)\n",
    "# This number is found in AREA attribute for the crime data and in PREC attribute for Police Stations data\n",
    "joined_df = crime_data_df.join(la_police_stations_df, crime_data_df[\"AREA\"] == la_police_stations_df[\"PREC\"], how=\"inner\")\n",
    "\n",
    "# Calculate the distance of the crime from the police department responsible for the specific area\n",
    "# The distance is calculated in km\n",
    "distance_from_pd_df = joined_df.withColumn(\"distance_km\", ST_DistanceSphere(\"cd_geometry\", \"pd_geometry\")/1000) \\\n",
    "                .groupBy(\"DIVISION\") \\\n",
    "                .agg(round(avg(\"distance_km\"), 1).alias(\"average distance\"), count(\"*\").alias(\"#\")) \\\n",
    "                .orderBy(desc(\"#\"))\n",
    "\n",
    "distance_from_pd_df.show()\n",
    "\n",
    "print(f\"Elapsed time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e700fb8-b556-4c5a-99eb-fdc7b10fb400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
